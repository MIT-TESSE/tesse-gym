{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasure Hunt Challenge\n",
    "\n",
    "This notebook demonstrates using [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) Proximal Policy Optimization to train a CNN-LSTM agent for the GOSEEK-Challenge. An agent must find as many treasures, placed around a TESSE environment, as possible in the alloted time.\n",
    "\n",
    "`tesse_gym` allows for interface customizations, some of which are demonstrated here. Specifically, this notebook contains an example of using combined rgb, segmentation, depth, and pose as the agent's observation.\n",
    "\n",
    "__Contents__\n",
    "- [Configure Environment](#Configuration)\n",
    "- [Define Model](#Define-the-Model)\n",
    "- [Train Model](#Train-the-Model)\n",
    "- [Visualize Results](#Visualize-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from tesse.msgs import *\n",
    "\n",
    "from tesse_gym.tasks.goseek import MultiModalGoSeek\n",
    "from tesse_gym import get_network_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set TESSE build path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path.home() / 'tess/builds/goseek/v0.0.2/goseek-0.0.2.x86_64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = True\n",
    "\n",
    "total_timesteps = 5000000\n",
    "scene_id = [1, 2, 4, 5]  # holdout scenes 3, 6\n",
    "success_dist = 2\n",
    "n_targets = [30, 30, 30, 30]\n",
    "max_steps = 400\n",
    "n_environments = 4\n",
    "target_found_reward = 2\n",
    "step_rate = 20\n",
    "\n",
    "if training:\n",
    "    VecEnv = SubprocVecEnv  \n",
    "else:\n",
    "    VecEnv = DummyVecEnv  \n",
    "    n_environments = 1\n",
    "        \n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = MultiModalGoSeek(filename, \n",
    "                                   network_config=get_network_config(worker_id=rank),\n",
    "                                   n_targets=n_targets[rank],\n",
    "                                   success_dist=success_dist,\n",
    "                                   max_steps=max_steps,\n",
    "                                   step_rate=step_rate,\n",
    "                                   scene_id=scene_id[rank],\n",
    "                                   target_found_reward=target_found_reward)\n",
    "            return env\n",
    "        return _thunk\n",
    "    \n",
    "    return VecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "The following network assumes an observation of RGB, segmentation, and depth images along with the agent's relative pose. Images are processed using the Stable-Baseline default CNN. The resulting feature vector is concatenated with the pose vector and fed into an LSTM (defined when we initialize PPO\n",
    "\n",
    "The OpenAI Gym [dictionary space](https://github.com/openai/gym/blob/master/gym/spaces/dict.py) is not supported, so we'll flatten the images into one vector and concatenate that with pose. This "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import nature_cnn\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from stable_baselines.a2c.utils import conv, linear, conv_to_fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_observation(obs):\n",
    "    \"\"\" Decode observation numpy array into images and pose.\n",
    "    TODO: don't hardcode this\n",
    "    \n",
    "    Args:\n",
    "        observation (np.ndarray): 1D observation array.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Images and pose tensors. \"\"\"\n",
    "    imgs = obs[:, :-3]\n",
    "    pose = obs[:, -3:]\n",
    "    \n",
    "    imgs = imgs.reshape(-1, 240, 320, 5)\n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_observation_tensor(observation):\n",
    "    \"\"\" Decode observation tensorflow Tensor into images and pose.\n",
    "    \n",
    "    Args:\n",
    "        observation (tf.Tensor): 1D observation tensor.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, tf.Tensor]: Images and pose tensors. \"\"\"\n",
    "    imgs = tf.reshape(observation[:, :-3], shape=(-1, 240, 320, 5))\n",
    "    pose = observation[:, -3:]\n",
    "    \n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(scaled_images, **kwargs):\n",
    "    \"\"\" Stable Baselines default cnn with batch norm \"\"\"\n",
    "    activ = tf.nn.relu\n",
    "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=8, stride=4, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=4, stride=2, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = activ(conv(layer_2, 'c3', n_filters=64, filter_size=3, stride=1, init_scale=np.sqrt(2), **kwargs))\n",
    "    layer_3 = conv_to_fc(layer_3)\n",
    "    return activ(linear(layer_3, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(raw_observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    imgs, pose = postprocess_observation_tensor(raw_observation)\n",
    "    image_features = cnn(imgs)\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(CnnLstmPolicy, env, verbose=1, tensorboard_log=\"./tensorboard/\", \n",
    "             nminibatches=2,\n",
    "             gamma=0.995,\n",
    "             learning_rate=0.00025,\n",
    "             policy_kwargs=policy_kwargs\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints\n",
    "This will save intermediate checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path('results/stable-baselines-ppo-1')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_checkpoint_callback(local_vars,  global_vars):\n",
    "    total_updates = local_vars['update'] \n",
    "    if total_updates % 100 == 0:\n",
    "        local_vars[\"self\"].save(str(log_dir / f'{total_updates:09d}.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=total_timesteps, callback=save_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ''\n",
    "model = PPO2.load(str(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "imgs, pose = postprocess_observation(obs)\n",
    "lstm_state = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(imgs[0, ..., :3])\n",
    "ax[1].imshow(imgs[0, ..., 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "fig, ax = plt.subplots()\n",
    "n_train_envs = model.act_model.initial_state.shape[0]\n",
    "\n",
    "for i in range(400):\n",
    "    actions, lstm_state = model.predict(np.repeat(obs, n_train_envs, 0), \n",
    "                                        state=lstm_state, \n",
    "                                        deterministic=False)\n",
    "    \n",
    "    action = actions[0]\n",
    "    obs, reward, done, _ = env.step([action])\n",
    "    done = done[0]\n",
    "    \n",
    "    plt.cla()\n",
    "    imgs, pose = postprocess_observation(obs)\n",
    "    \n",
    "    # display RGB image\n",
    "    ax.imshow((255*imgs[0, ..., :3]).astype(np.uint8))\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "obs = env.reset()\n",
    "imgs, pose = postprocess_observation(obs)\n",
    "lstm_state = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tess]",
   "language": "python",
   "name": "conda-env-tess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
