{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasure Hunt Challenge\n",
    "\n",
    "This notebook demonstrates using [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) Proximal Policy Optimization to train a CNN-LSTM agent for the GOSEEK-Challenge. An agent must find as many treasures placed around a simulated environment as possible in the alloted time.\n",
    "\n",
    "`tesse_gym` allows for interface customizations, some of which are demonstrated here. Specifically, this notebook contains an example of using combined rgb, segmentation, depth, and pose as the agent's observation.\n",
    "\n",
    "__Contents__\n",
    "- [Configure Environment](#Configuration)\n",
    "- [Define Model](#Define-the-Model)\n",
    "- [Train Model](#Train-the-Model)\n",
    "- [Visualize Results](#Visualize-Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from gym import spaces\n",
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "\n",
    "from tesse.msgs import *\n",
    "\n",
    "from tesse_gym.tasks.goseek import GoSeekFullPerception\n",
    "from tesse_gym import get_network_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set build path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path.home() / 'tess/builds/goseek/v0.0.2/goseek-0.0.2.x86_64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_environments = 2\n",
    "total_timesteps = 5000000\n",
    "scene_id = [1, 2, 4, 5]  # holdout scenes 3, 6 for validation\n",
    "success_dist = 2\n",
    "n_targets = [30, 30, 30, 30]\n",
    "episode_length = 400\n",
    "target_found_reward = 2\n",
    "step_rate = 20\n",
    "\n",
    "\n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = GoSeekFullPerception(\n",
    "                str(filename),\n",
    "                network_config=get_network_config(worker_id=rank),\n",
    "                n_targets=n_targets[rank],\n",
    "                success_dist=success_dist,\n",
    "                episode_length=episode_length,\n",
    "                step_rate=step_rate,\n",
    "                scene_id=scene_id[rank],\n",
    "                target_found_reward=target_found_reward,\n",
    "            )\n",
    "            return env\n",
    "\n",
    "        return _thunk\n",
    "\n",
    "    return SubprocVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the Model \n",
    "The following network assumes an observation of RGB, segmentation, and depth images along with the agent's relative pose. Images are processed using the Stable-Baseline default CNN. The resulting feature vector is concatenated with the pose vector and fed into an LSTM (defined when we initialize PPO\n",
    "\n",
    "The OpenAI Gym [dictionary space](https://github.com/openai/gym/blob/master/gym/spaces/dict.py) is not supported, so we'll flatten the images into one vector and concatenate that with pose. This "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from stable_baselines.common.policies import nature_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_observations(observation, img_shape=(-1, 240, 320, 5)):\n",
    "    \"\"\" Decode observation vector into images and pose.\n",
    "    \n",
    "    Args:\n",
    "        observation (np.ndarray): 1D observationervation array.\n",
    "        img_shape (Tuple[int]): Shapes of all images stacked in (H, W, C).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]: Images and pose tensors.\n",
    "    \"\"\"\n",
    "    if isinstance(observation, np.ndarray):\n",
    "        imgs = observation[:, :-3].reshape(img_shape)\n",
    "    elif isinstance(observation, tf.Tensor):\n",
    "        imgs = tf.reshape(observation[:, :-3], img_shape)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Expected type `np.ndarray` or `tf.Tensor`, got: {type(observation)}\"\n",
    "        )\n",
    "\n",
    "    pose = observation[:, -3:]\n",
    "\n",
    "    return imgs, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_and_pose_network(observation, **kwargs):\n",
    "    \"\"\" Network to process image and pose data.\n",
    "    \n",
    "    Use the stable baselines nature_cnn to process images. The resulting\n",
    "    feature vector is then combined with the pose estimate and given to an\n",
    "    LSTM (LSTM not defined here).\n",
    "    \n",
    "    Args:\n",
    "        raw_observations (tf.Tensor): 1D tensor containing image and \n",
    "            pose data.\n",
    "        \n",
    "    Returns:\n",
    "        tf.Tensor: Feature vector. \n",
    "    \"\"\"\n",
    "    imgs, pose = decode_observations(observation)\n",
    "    image_features = nature_cnn(imgs)\n",
    "    return tf.concat((image_features, pose), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = {'cnn_extractor': image_and_pose_network}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(\n",
    "    CnnLstmPolicy,\n",
    "    env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    nminibatches=2,\n",
    "    gamma=0.995,\n",
    "    learning_rate=0.00025,\n",
    "    policy_kwargs=policy_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints\n",
    "This will save intermediate checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"results/goseek-ppo\")\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_checkpoint_callback(local_vars, global_vars):\n",
    "    total_updates = local_vars[\"update\"]\n",
    "    if total_updates % 1000 == 0:\n",
    "        local_vars[\"self\"].save(str(log_dir / f\"{total_updates:09d}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=total_timesteps, callback=save_checkpoint_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook`\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = ''\n",
    "assert MODEL_PATH, f\"Must give a model path!\"\n",
    "\n",
    "model = PPO2.load(str(MODEL_PATH))\n",
    "n_train_envs = model.act_model.initial_state.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "imgs, pose = decode_observations(obs)\n",
    "lstm_state = None\n",
    "\n",
    "assert (\n",
    "    n_train_envs % obs.shape[0] == 0\n",
    "), f\"The number of visualization environments must be a multiple of the training environments\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(imgs[0, ..., :3])\n",
    "ax[1].imshow(imgs[0, ..., 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i in range(max_steps):\n",
    "    actions, lstm_state = model.predict(\n",
    "        np.repeat(obs, n_train_envs // obs.shape[0], 0),\n",
    "        state=lstm_state,\n",
    "        deterministic=False,\n",
    "    )\n",
    "\n",
    "    action = actions[::obs.shape[0]]\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    plt.cla()\n",
    "    imgs, pose = decode_observations(obs)\n",
    "\n",
    "    # display RGB image\n",
    "    ax.imshow((255 * imgs[0, ..., :3]).astype(np.uint8))\n",
    "    fig.canvas.draw()\n",
    "\n",
    "obs = env.reset()\n",
    "imgs, pose = decode_observations(obs)\n",
    "lstm_state = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tess]",
   "language": "python",
   "name": "conda-env-tess-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
