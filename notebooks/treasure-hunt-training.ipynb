{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "Distribution authorized to U.S. Government agencies and their contractors. Other requests for this document shall be referred to the MIT Lincoln Laboratory Technology Office.\n",
    "\n",
    "This material is based upon work supported by the Under Secretary of Defense for Research and Engineering under Air Force Contract No. FA8702-15-D-0001. Any opinions, findings, conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the Under Secretary of Defense for Research and Engineering.\n",
    "\n",
    "Â© 2019 Massachusetts Institute of Technology.\n",
    "\n",
    "The software/firmware is provided to you on an As-Is basis\n",
    "\n",
    "Delivered to the U.S. Government with Unlimited Rights, as defined in DFARS Part 252.227-7013 or 7014 (Feb 2014). Notwithstanding any copyright notice, U.S. Government rights in this work are defined by DFARS 252.227-7013 or DFARS 252.227-7014 as detailed above. Use of this work other than as specifically authorized by the U.S. Government may violate any copyrights that exist in this work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treasure Hunt Challenge\n",
    "\n",
    "This notebook demonstrates using [Stable Baselines](https://stable-baselines.readthedocs.io/en/master/) Proximal Policy Optimization to train a CNN-LSTM agent on the TESSE Autonomous Treasure Hunt Challange. For this challange, an agent must find as many 'treasures', placed around a TESSE environment, as possible in the alloted time (currently 100 timesteps).\n",
    "\n",
    "`tesse_gym` allows for interface customizations, some of which are demonstrated here. Specifically, this notebook contains an example of changing the agent's observation from the default rgb image to combined rgb, segmentation, and depth. Additional configurations such as the reward function and action space may also be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.policies import CnnLstmPolicy\n",
    "from stable_baselines.common.vec_env import SubprocVecEnv, VecVideoRecorder, DummyVecEnv\n",
    "from stable_baselines import PPO2\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "from tesse.msgs import *\n",
    "from tesse_gym.treasure_hunt import TreasureHunt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Path to TESSE build\n",
    "\n",
    "If you're on the LLAN, you can grab the build at: `\\\\group104\\share\\users\\GriffithDan\\public\\tess\\builds\\tesse_multiscene_v0.5.0_linux.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = Path.home() / 'tess/builds/v0.5.0/tesse_multiscene_v0.5.0_linux.x86_64'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure Cameras\n",
    "Below is an example of adjusting camera resolution, field of view, clipping planes, and position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_cameras(tesse_interface):\n",
    "    tesse_interface.env.request(SetCameraParametersRequest(camera=Camera.RGB_LEFT, \n",
    "                                                           height_in_pixels=240, \n",
    "                                                           width_in_pixels=320, \n",
    "                                                           field_of_view=45, \n",
    "                                                           near_clip_plane=0.05, \n",
    "                                                           far_clip_plane=50))\n",
    "    tesse_interface.env.request(SetCameraParametersRequest(camera=Camera.SEGMENTATION, \n",
    "                                                           height_in_pixels=240, \n",
    "                                                           width_in_pixels=320, \n",
    "                                                           field_of_view=45, \n",
    "                                                           near_clip_plane=0.05, \n",
    "                                                           far_clip_plane=50))\n",
    "    tesse_interface.env.request(SetCameraParametersRequest(camera=Camera.DEPTH, \n",
    "                                                           height_in_pixels=240, \n",
    "                                                           width_in_pixels=320, \n",
    "                                                           field_of_view=45, \n",
    "                                                           near_clip_plane=0.05, \n",
    "                                                           far_clip_plane=50))\n",
    "    tesse_interface.env.request(SetCameraPositionRequest(camera=Camera.RGB_LEFT, \n",
    "                                                         x=0, \n",
    "                                                         y=0, \n",
    "                                                         z=-0.1))\n",
    "    tesse_interface.env.request(SetCameraPositionRequest(camera=Camera.SEGMENTATION, \n",
    "                                                         x=0, \n",
    "                                                         y=0, \n",
    "                                                         z=-0.1))\n",
    "    tesse_interface.env.request(SetCameraPositionRequest(camera=Camera.DEPTH, \n",
    "                                                         x=0, \n",
    "                                                         y=0, \n",
    "                                                         z=-0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjust agent observation\n",
    "Override observation methods to give RGB, segmentation, and depth information to agent.  This requires\n",
    "\n",
    "1. Defining the observation space\n",
    "2. Overriding `form_agent_observation()`\n",
    "3. Overriding `observe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tesse_gym.treasure_hunt import HuntMode\n",
    "from gym import spaces\n",
    "class RGBSegDepthInput(TreasureHunt):   \n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        \"\"\" This must be defined for custom observations. \"\"\"\n",
    "        return spaces.Box(0, 255, dtype=np.float32, shape=(240, 320, 7))\n",
    "    \n",
    "    def form_agent_observation(self, tesse_data):\n",
    "        \"\"\" Create the agent's observation from a TESSE data response. \"\"\"\n",
    "        eo, seg, depth = tesse_data.images\n",
    "        observation = np.concatenate((eo / 255.0, \n",
    "                                      seg / 255.0, \n",
    "                                      depth[..., np.newaxis]), axis=-1)\n",
    "        return observation\n",
    "    \n",
    "    def observe(self):\n",
    "        cameras = [\n",
    "            (Camera.RGB_LEFT, Compression.OFF, Channels.THREE),\n",
    "            (Camera.SEGMENTATION, Compression.OFF, Channels.THREE),\n",
    "            (Camera.DEPTH, Compression.OFF, Channels.THREE)\n",
    "        ]\n",
    "        agent_data = self.env.request(DataRequest(metadata=True, cameras=cameras))           \n",
    "        return agent_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define logging directory and callback function to save checkpoints\n",
    "This will save intermediate checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path('results/testing/')\n",
    "log_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_checkpoint_callback(local_vars,  global_vars):\n",
    "    total_updates = local_vars['update'] \n",
    "    if total_updates % 50 == 0:\n",
    "        local_vars[\"self\"].save(str(log_dir / f'{total_updates:09d}.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting environment parameters\n",
    "\n",
    "\n",
    "__Note__: For ease of debugging this uses `DummyVecEnv` with 1 environment. For actual exeriments change this to `SubprocVecEnv` with multiple environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = 6000000\n",
    "scene_id = 5\n",
    "success_dist = 2\n",
    "n_targets = 50\n",
    "max_steps = 100\n",
    "restart_on_collision = False\n",
    "n_environments = 1\n",
    "    \n",
    "def make_unity_env(filename, num_env):\n",
    "    \"\"\" Create a wrapped Unity environment. \"\"\"\n",
    "    def make_env(rank):\n",
    "        def _thunk():\n",
    "            env = RGBSegDepthInput(filename, \n",
    "                                'localhost',\n",
    "                                'localhost', \n",
    "                                worker_id=rank, \n",
    "                                step_rate=30,\n",
    "                                scene_id=scene_id)\n",
    "            return env\n",
    "        return _thunk\n",
    "    \n",
    "    return DummyVecEnv([make_env(i) for i in range(num_env)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next, we launch environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = make_unity_env(filename, n_environments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the agent model for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2(CnnLstmPolicy, env, verbose=1, tensorboard_log=\"./tensorboard/\", nminibatches=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=total_timesteps, callback=save_checkpoint_callback)\n",
    "model.save(\"the.policy\")  # save finals policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a video\n",
    "\n",
    "Demonstrates loading the model and executing it to construct a video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO2.load('the.policy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_length = 500\n",
    "\n",
    "video_env = VecVideoRecorder(env,\n",
    "                             video_folder='videos',\n",
    "                             record_video_trigger=lambda x: x == 0,\n",
    "                             video_length=video_length,\n",
    "                             name_prefix='test-1'\n",
    "                            )\n",
    "\n",
    "obs = video_env.reset()\n",
    "for _ in range(video_length + 1):\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, _, _, _ = video_env.step(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tess_gym]",
   "language": "python",
   "name": "conda-env-tess_gym-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
